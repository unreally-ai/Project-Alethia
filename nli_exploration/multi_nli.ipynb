{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset from huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cas/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset multi_nli (/home/cas/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39)\n",
      "100%|██████████| 3/3 [00:00<00:00,  8.81it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"multi_nli\")\n",
    "train = dataset['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess\n",
    "Apply tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "# load the auto tokenizer from huggingface\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7592, 1010, 7592, 2088, 999, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "returns a dict with 3 items:\n",
    "- numbers representing the tokens\n",
    "- indicator to which sequence a token belongs to\n",
    "- indicates whether a token should be masked or not\n",
    "\"\"\"\n",
    "tokenizer(\"hello, hello world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'promptID': 31193,\n",
       " 'pairID': '31193n',\n",
       " 'premise': 'Conceptually cream skimming has two basic dimensions - product and geography.',\n",
       " 'premise_binary_parse': '( ( Conceptually ( cream skimming ) ) ( ( has ( ( ( two ( basic dimensions ) ) - ) ( ( product and ) geography ) ) ) . ) )',\n",
       " 'premise_parse': '(ROOT (S (NP (JJ Conceptually) (NN cream) (NN skimming)) (VP (VBZ has) (NP (NP (CD two) (JJ basic) (NNS dimensions)) (: -) (NP (NN product) (CC and) (NN geography)))) (. .)))',\n",
       " 'hypothesis': 'Product and geography are what make cream skimming work. ',\n",
       " 'hypothesis_binary_parse': '( ( ( Product and ) geography ) ( ( are ( what ( make ( cream ( skimming work ) ) ) ) ) . ) )',\n",
       " 'hypothesis_parse': '(ROOT (S (NP (NN Product) (CC and) (NN geography)) (VP (VBP are) (SBAR (WHNP (WP what)) (S (VP (VBP make) (NP (NP (NN cream)) (VP (VBG skimming) (NP (NN work)))))))) (. .)))',\n",
       " 'genre': 'government',\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "# load the tranformer to create embeddings\n",
    "st = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "embeds = st.encode([train[0]['premise'], train[0]['hypothesis']], convert_to_tensor=True)\n",
    "# creates embeddings for both sentences in the array and returns array of size 2\n",
    "\n",
    "#util.pytorch_cos_sim(embeds[0], embeds[1])\n",
    "x = torch.cat((embeds[0], embeds[1]), -1)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset multi_nli (/home/cas/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39)\n"
     ]
    }
   ],
   "source": [
    "# first ten rows for testing\n",
    "sample = load_dataset(\"multi_nli\", split='train[:10]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n"
     ]
    },
    {
     "ename": "ArrowInvalid",
     "evalue": "Column 10 named embed expected length 10 but got length 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [82], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m#embeds = torch.cat((embeds[0], embeds[1]))\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membed\u001b[39m\u001b[38;5;124m'\u001b[39m: [torch\u001b[38;5;241m.\u001b[39mcat((embeds[\u001b[38;5;241m0\u001b[39m], embeds[\u001b[38;5;241m1\u001b[39m]), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)]}\n\u001b[0;32m----> 8\u001b[0m tk_train \u001b[38;5;241m=\u001b[39m \u001b[43msample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:2826\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   <a href='file:///home/cas/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=2822'>2823</a>\u001b[0m disable_tqdm \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled()\n\u001b[1;32m   <a href='file:///home/cas/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=2824'>2825</a>\u001b[0m \u001b[39mif\u001b[39;00m num_proc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m num_proc \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m-> <a href='file:///home/cas/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=2825'>2826</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_single(\n\u001b[1;32m   <a href='file:///home/cas/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=2826'>2827</a>\u001b[0m         function\u001b[39m=\u001b[39;49mfunction,\n\u001b[1;32m   <a href='file:///home/cas/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=2827'>2828</a>\u001b[0m         with_indices\u001b[39m=\u001b[39;49mwith_indices,\n\u001b[1;32m   <a href='file:///home/cas/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=2828'>2829</a>\u001b[0m         with_rank\u001b[39m=\u001b[39;49mwith_rank,\n\u001b[1;32m   <a href='file:///home/cas/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=2829'>2830</a>\u001b[0m         input_columns\u001b[39m=\u001b[39;49minput_columns,\n\u001b[1;32m   <a href='file:///home/cas/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=2830'>2831</a>\u001b[0m         batched\u001b[39m=\u001b[39;49mbatched,\n\u001b[1;32m   <a href='file:///home/cas/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=2831'>2832</a>\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m   <a href='file:///home/cas/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=2832'>2833</a>\u001b[0m         drop_last_batch\u001b[39m=\u001b[39;49mdrop_last_batch,\n\u001b[1;32m   <a href='file:///home/cas/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=2833'>2834</a>\u001b[0m         remove_columns\u001b[39m=\u001b[39;49mremove_columns,\n\u001b[1;32m   <a href='file:///home/cas/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=2834'>2835</a>\u001b[0m         keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[1;32m   <a href='file:///home/cas/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=2835'>2836</a>\u001b[0m         load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[1;32m   <a href='file:///home/cas/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=2836'>2837</a>\u001b[0m         cache_file_name\u001b[39m=\u001b[39;49mcache_file_name,\n\u001b[1;32m   <a href='file:///home/cas/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=2837'>2838</a>\u001b[0m         writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[1;32m   <a href='file:///home/cas/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=2838'>2839</a>\u001b[0m         features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m   <a href='file:///home/cas/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=2839'>2840</a>\u001b[0m         disable_nullable\u001b[39m=\u001b[39;49mdisable_nullable,\n\u001b[1;32m   <a href='file:///home/cas/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=2840'>2841</a>\u001b[0m         fn_kwargs\u001b[39m=\u001b[39;49mfn_kwargs,\n\u001b[1;32m   <a href='file:///home/cas/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=2841'>2842</a>\u001b[0m         new_fingerprint\u001b[39m=\u001b[39;49mnew_fingerprint,\n\u001b[1;32m   <a href='file:///home/cas/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=2842'>2843</a>\u001b[0m         disable_tqdm\u001b[39m=\u001b[39;49mdisable_tqdm,\n\u001b[1;32m   <a href='file:///home/cas/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=2843'>2844</a>\u001b[0m         desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[1;32m   <a href='file:///home/cas/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=2844'>2845</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///home/cas/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=2845'>2846</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/cas/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=2847'>2848</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mformat_cache_file_name\u001b[39m(cache_file_name, rank):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:562\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/cas/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=559'>560</a>\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///home/cas/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=560'>561</a>\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/cas/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=561'>562</a>\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/cas/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=562'>563</a>\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    <a href='file:///home/cas/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=563'>564</a>\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[1;32m    <a href='file:///home/cas/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=564'>565</a>\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:529\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/cas/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=521'>522</a>\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    <a href='file:///home/cas/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=522'>523</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    <a href='file:///home/cas/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=523'>524</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    <a href='file:///home/cas/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=524'>525</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    <a href='file:///home/cas/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=525'>526</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    <a href='file:///home/cas/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=526'>527</a>\u001b[0m }\n\u001b[1;32m    <a href='file:///home/cas/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=527'>528</a>\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/cas/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=528'>529</a>\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/cas/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=529'>530</a>\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    <a href='file:///home/cas/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=530'>531</a>\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/fingerprint.py:480\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/cas/.local/lib/python3.10/site-packages/datasets/fingerprint.py?line=475'>476</a>\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[1;32m    <a href='file:///home/cas/.local/lib/python3.10/site-packages/datasets/fingerprint.py?line=477'>478</a>\u001b[0m \u001b[39m# Call actual function\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/cas/.local/lib/python3.10/site-packages/datasets/fingerprint.py?line=479'>480</a>\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/cas/.local/lib/python3.10/site-packages/datasets/fingerprint.py?line=481'>482</a>\u001b[0m \u001b[39m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/cas/.local/lib/python3.10/site-packages/datasets/fingerprint.py?line=483'>484</a>\u001b[0m \u001b[39mif\u001b[39;00m inplace:  \u001b[39m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:3264\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\u001b[0m\n\u001b[1;32m   <a href='file:///home/cas/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=3261'>3262</a>\u001b[0m                 writer\u001b[39m.\u001b[39mwrite_table(batch)\n\u001b[1;32m   <a href='file:///home/cas/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=3262'>3263</a>\u001b[0m             \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/cas/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=3263'>3264</a>\u001b[0m                 writer\u001b[39m.\u001b[39;49mwrite_batch(batch)\n\u001b[1;32m   <a href='file:///home/cas/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=3264'>3265</a>\u001b[0m \u001b[39mif\u001b[39;00m update_data \u001b[39mand\u001b[39;00m writer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/cas/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=3265'>3266</a>\u001b[0m     writer\u001b[39m.\u001b[39mfinalize()  \u001b[39m# close_stream=bool(buf_writer is None))  # We only close if we are writing in a file\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_writer.py:554\u001b[0m, in \u001b[0;36mArrowWriter.write_batch\u001b[0;34m(self, batch_examples, writer_batch_size)\u001b[0m\n\u001b[1;32m    <a href='file:///home/cas/.local/lib/python3.10/site-packages/datasets/arrow_writer.py?line=551'>552</a>\u001b[0m         inferred_features[col] \u001b[39m=\u001b[39m typed_sequence\u001b[39m.\u001b[39mget_inferred_type()\n\u001b[1;32m    <a href='file:///home/cas/.local/lib/python3.10/site-packages/datasets/arrow_writer.py?line=552'>553</a>\u001b[0m schema \u001b[39m=\u001b[39m inferred_features\u001b[39m.\u001b[39marrow_schema \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpa_writer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mschema\n\u001b[0;32m--> <a href='file:///home/cas/.local/lib/python3.10/site-packages/datasets/arrow_writer.py?line=553'>554</a>\u001b[0m pa_table \u001b[39m=\u001b[39m pa\u001b[39m.\u001b[39;49mTable\u001b[39m.\u001b[39;49mfrom_arrays(arrays, schema\u001b[39m=\u001b[39;49mschema)\n\u001b[1;32m    <a href='file:///home/cas/.local/lib/python3.10/site-packages/datasets/arrow_writer.py?line=554'>555</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite_table(pa_table, writer_batch_size)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyarrow/table.pxi:3592\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.from_arrays\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyarrow/table.pxi:2785\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.validate\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyarrow/error.pxi:100\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowInvalid\u001b[0m: Column 10 named embed expected length 10 but got length 1"
     ]
    }
   ],
   "source": [
    "# map to tokenize & embed the entire dataset\n",
    "def tokenize(batch):\n",
    "    embeds = st.encode([batch['premise'], batch['hypothesis']], convert_to_tensor=True)\n",
    "    #embeds = torch.cat((embeds[0], embeds[1]))\n",
    "    return {'embed': torch.cat((embeds[0], embeds[1]), -1)}\n",
    "\n",
    "\n",
    "tk_train = sample.map(\n",
    "    tokenize, \n",
    "    batched=True\n",
    ")\n",
    "#remove_columns=sample.column_names,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['embed'],\n",
       "    num_rows: 768\n",
       "})"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set dataset format to pytorch\n",
    "tk_train.set_format(type=\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"label\"])\n",
    "tk_train.format['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': tensor(1),\n",
       " 'input_ids': tensor([  101, 17158,  2135,  6949,  8301, 25057,  2038,  2048,  3937,  9646,\n",
       "          1011,  4031,  1998, 10505,  1012,   102]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(tk_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "# load pretrained GloVe embeddings\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "max_words = 20\n",
    "embed_len=300\n",
    "global_vectors = GloVe(name='840B', dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# load to GPU for speed up\n",
    "device = 'cuda'\n",
    "# takes string, returns 6000 dim GloVe vector\n",
    "def to_vector(s):\n",
    "    X = tokenizer(s[0])\n",
    "    # fill / cut tokens to max size\n",
    "    if len(X) < max_words:\n",
    "        X = X+[\"\"]*(max_words-len(X))\n",
    "    else:\n",
    "        X = X[:max_words]\n",
    "\n",
    "    X_tensor = torch.zeros(1, max_words, embed_len).to(device)\n",
    "    for i, j in enumerate(X):\n",
    "        X_tensor[0][i] = global_vectors.get_vecs_by_tokens(j)\n",
    "    return(X_tensor.reshape(1, -1))\n",
    "\n",
    "def combined_tensor(row):\n",
    "    x = to_vector(row['premise'])\n",
    "    y = to_vector(row['hypothesis'])\n",
    "\n",
    "    #return torch.cat((x, y), 1)\n",
    "    return {\n",
    "        'vector': torch.cat((x, y), 1),\n",
    "        'label': row['label']\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vector': tensor([[-0.3423, -0.0060, -0.4845,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "        device='cuda:0'),\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_tensor(\n",
    "    dataset['train'][0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset multi_nli (/home/cas/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39)\n"
     ]
    }
   ],
   "source": [
    "sample_data = load_dataset(\"multi_nli\", split='train[:1]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_datta = sample_data.with_transform(\n",
    "    combined_tensor,\n",
    "    columns=['premise', 'hypothesis', 'label'],\n",
    "    output_all_columns=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vector': tensor([-0.0168, -0.2356,  0.0769,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        device='cuda:0'),\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the with_transform applies the custom preprocessing function on-the-fly\n",
    "tf_datta.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map tokenizer to data\n",
    "tk_data = sample_data.map(\n",
    "    # creates a 12k GloVe of premise + hypothesis\n",
    "    combined_tensor,\n",
    "    # speed up by only keeping important columns\n",
    "    input_columns=['premise', 'hypothesis', 'label'],\n",
    "    remove_columns=sample_data.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'vector'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The objectives were realistic.', 'I enjoyed talking with you and must go home. ', 'One of the monsters cried out in English.', 'Road camping trips will be the last thing you have left.'] \n",
      "\n",
      " ['The objectives of our research were to (1) define and describe the characteristics of a worldclass finance organization, (2) identify the factors that are essential for finance organizations to improve their financial management and move towards worldclass standards, and (3) provide case studies which illustrate the efforts of leading finance organizations from private sector companies and state governments to improve their financial management and the overall performance of their organizations.', 'yeah yeah well i need to run i enjoyed talking to you', 'One of the sharp-toothed monsters cried out in a low desert language.', \"and then all you'll have will be road camping trips you know let's just go out to the mountains with the car honey and uh\"] \n",
      "\n",
      " tensor([1, 1, 2, 0])\n"
     ]
    }
   ],
   "source": [
    "# pytorch data loading\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def load_dataloader(split: str, batch: int):\n",
    "    ds = dataset[split].with_format('torch')\n",
    "    dl = DataLoader(ds, batch_size=batch, shuffle=True)   \n",
    "    return dl\n",
    "\n",
    "train_dl = load_dataloader('train', 4)\n",
    "\n",
    "\"\"\"\n",
    "Since we have batch=4, we get 4 datapoints. \n",
    "Each index of the label tensor belongs to one text pair.\n",
    "\"\"\"\n",
    "\n",
    "# test and display dataloader for 1 batch\n",
    "for idx, batch in enumerate(train_dl):\n",
    "    print(batch['hypothesis'], '\\n\\n', batch['premise'], '\\n\\n', batch['label'])\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
