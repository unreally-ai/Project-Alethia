{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset from huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset fever (/home/cas/.cache/huggingface/datasets/fever/v1.0/1.0.0/7f8936e0558704771b08c7ce9cc202071b29a0050603374507ba61d23c00a58e)\n",
      "100%|██████████| 6/6 [00:00<00:00, 36.54it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'label', 'claim', 'evidence_annotation_id', 'evidence_id', 'evidence_wiki_url', 'evidence_sentence_id'],\n",
       "        num_rows: 311431\n",
       "    })\n",
       "    labelled_dev: Dataset({\n",
       "        features: ['id', 'label', 'claim', 'evidence_annotation_id', 'evidence_id', 'evidence_wiki_url', 'evidence_sentence_id'],\n",
       "        num_rows: 37566\n",
       "    })\n",
       "    unlabelled_dev: Dataset({\n",
       "        features: ['id', 'label', 'claim', 'evidence_annotation_id', 'evidence_id', 'evidence_wiki_url', 'evidence_sentence_id'],\n",
       "        num_rows: 19998\n",
       "    })\n",
       "    unlabelled_test: Dataset({\n",
       "        features: ['id', 'label', 'claim', 'evidence_annotation_id', 'evidence_id', 'evidence_wiki_url', 'evidence_sentence_id'],\n",
       "        num_rows: 19998\n",
       "    })\n",
       "    paper_dev: Dataset({\n",
       "        features: ['id', 'label', 'claim', 'evidence_annotation_id', 'evidence_id', 'evidence_wiki_url', 'evidence_sentence_id'],\n",
       "        num_rows: 18999\n",
       "    })\n",
       "    paper_test: Dataset({\n",
       "        features: ['id', 'label', 'claim', 'evidence_annotation_id', 'evidence_id', 'evidence_wiki_url', 'evidence_sentence_id'],\n",
       "        num_rows: 18567\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "#dataset = load_dataset(\"multi_nli\")\n",
    "dataset = load_dataset(\"fever\", 'v1.0')\n",
    "\n",
    "\"\"\" \n",
    "'id', \n",
    "'label',\n",
    "'claim', \n",
    "'evidence_annotation_id',\n",
    "'evidence_id',\n",
    "'evidence_wiki_url',\n",
    "'evidence_sentence_id'\n",
    "\"\"\"\n",
    "train = dataset[\"train\"]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration wiki-44edd95e7c123e85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/wiki to /home/cas/.cache/huggingface/datasets/json/wiki-44edd95e7c123e85/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 4232.40it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 361.86it/s]\n"
     ]
    },
    {
     "ename": "DatasetGenerationError",
     "evalue": "An error occurred while generating the dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/builder.py:1831\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[0m\n\u001b[1;32m   1825\u001b[0m     writer \u001b[39m=\u001b[39m writer_class(\n\u001b[1;32m   1826\u001b[0m         features\u001b[39m=\u001b[39mwriter\u001b[39m.\u001b[39m_features,\n\u001b[1;32m   1827\u001b[0m         path\u001b[39m=\u001b[39mfpath\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39mSSSSS\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mshard_id\u001b[39m:\u001b[39;00m\u001b[39m05d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39mJJJJJ\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mjob_id\u001b[39m:\u001b[39;00m\u001b[39m05d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   1828\u001b[0m         storage_options\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fs\u001b[39m.\u001b[39mstorage_options,\n\u001b[1;32m   1829\u001b[0m         embed_local_files\u001b[39m=\u001b[39membed_local_files,\n\u001b[1;32m   1830\u001b[0m     )\n\u001b[0;32m-> 1831\u001b[0m writer\u001b[39m.\u001b[39;49mwrite_table(table)\n\u001b[1;32m   1832\u001b[0m num_examples_progress_update \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(table)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_writer.py:567\u001b[0m, in \u001b[0;36mArrowWriter.write_table\u001b[0;34m(self, pa_table, writer_batch_size)\u001b[0m\n\u001b[1;32m    566\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_writer(inferred_schema\u001b[39m=\u001b[39mpa_table\u001b[39m.\u001b[39mschema)\n\u001b[0;32m--> 567\u001b[0m pa_table \u001b[39m=\u001b[39m table_cast(pa_table, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_schema)\n\u001b[1;32m    568\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_local_files:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/table.py:2282\u001b[0m, in \u001b[0;36mtable_cast\u001b[0;34m(table, schema)\u001b[0m\n\u001b[1;32m   2281\u001b[0m \u001b[39mif\u001b[39;00m table\u001b[39m.\u001b[39mschema \u001b[39m!=\u001b[39m schema:\n\u001b[0;32m-> 2282\u001b[0m     \u001b[39mreturn\u001b[39;00m cast_table_to_schema(table, schema)\n\u001b[1;32m   2283\u001b[0m \u001b[39melif\u001b[39;00m table\u001b[39m.\u001b[39mschema\u001b[39m.\u001b[39mmetadata \u001b[39m!=\u001b[39m schema\u001b[39m.\u001b[39mmetadata:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/table.py:2240\u001b[0m, in \u001b[0;36mcast_table_to_schema\u001b[0;34m(table, schema)\u001b[0m\n\u001b[1;32m   2239\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39msorted\u001b[39m(table\u001b[39m.\u001b[39mcolumn_names) \u001b[39m!=\u001b[39m \u001b[39msorted\u001b[39m(features):\n\u001b[0;32m-> 2240\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt cast\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mtable\u001b[39m.\u001b[39mschema\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mto\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfeatures\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mbecause column names don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt match\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   2241\u001b[0m arrays \u001b[39m=\u001b[39m [cast_array_to_feature(table[name], feature) \u001b[39mfor\u001b[39;00m name, feature \u001b[39min\u001b[39;00m features\u001b[39m.\u001b[39mitems()]\n",
      "\u001b[0;31mValueError\u001b[0m: Couldn't cast\n_data_files: list<item: struct<filename: string>>\n  child 0, item: struct<filename: string>\n      child 0, filename: string\n_fingerprint: string\n_format_columns: null\n_format_kwargs: struct<>\n_format_type: null\n_output_all_columns: bool\n_split: string\nto\n{'builder_name': Value(dtype='string', id=None), 'citation': Value(dtype='string', id=None), 'config_name': Value(dtype='string', id=None), 'dataset_size': Value(dtype='int64', id=None), 'description': Value(dtype='string', id=None), 'download_checksums': {'https://fever.ai/download/fever/wiki-pages.zip': {'num_bytes': Value(dtype='int64', id=None), 'checksum': Value(dtype='string', id=None)}}, 'download_size': Value(dtype='int64', id=None), 'features': {'id': {'dtype': Value(dtype='string', id=None), '_type': Value(dtype='string', id=None)}, 'text': {'dtype': Value(dtype='string', id=None), '_type': Value(dtype='string', id=None)}, 'lines': {'dtype': Value(dtype='string', id=None), '_type': Value(dtype='string', id=None)}}, 'homepage': Value(dtype='string', id=None), 'license': Value(dtype='string', id=None), 'size_in_bytes': Value(dtype='int64', id=None), 'splits': {'wikipedia_pages': {'name': Value(dtype='string', id=None), 'num_bytes': Value(dtype='int64', id=None), 'num_examples': Value(dtype='int64', id=None), 'shard_lengths': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'dataset_name': Value(dtype='string', id=None)}}, 'version': {'version_str': Value(dtype='string', id=None), 'major': Value(dtype='int64', id=None), 'minor': Value(dtype='int64', id=None), 'patch': Value(dtype='int64', id=None)}}\nbecause column names don't match",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDatasetGenerationError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m/home/cas/Documents/coding/python/Aletheia/nli_exploration/multi_nli.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/cas/Documents/coding/python/Aletheia/nli_exploration/multi_nli.ipynb#X36sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m\"\"\" \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/cas/Documents/coding/python/Aletheia/nli_exploration/multi_nli.ipynb#X36sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m'id', 'text', 'lines'\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/cas/Documents/coding/python/Aletheia/nli_exploration/multi_nli.ipynb#X36sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/cas/Documents/coding/python/Aletheia/nli_exploration/multi_nli.ipynb#X36sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m#wiki = load_dataset(\"fever\", \"wiki_pages\")\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/cas/Documents/coding/python/Aletheia/nli_exploration/multi_nli.ipynb#X36sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m wiki \u001b[39m=\u001b[39m load_dataset(\u001b[39m'\u001b[39;49m\u001b[39mdata/wiki\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/load.py:1757\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, **config_kwargs)\u001b[0m\n\u001b[1;32m   1754\u001b[0m try_from_hf_gcs \u001b[39m=\u001b[39m path \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n\u001b[1;32m   1756\u001b[0m \u001b[39m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m builder_instance\u001b[39m.\u001b[39;49mdownload_and_prepare(\n\u001b[1;32m   1758\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1759\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1760\u001b[0m     ignore_verifications\u001b[39m=\u001b[39;49mignore_verifications,\n\u001b[1;32m   1761\u001b[0m     try_from_hf_gcs\u001b[39m=\u001b[39;49mtry_from_hf_gcs,\n\u001b[1;32m   1762\u001b[0m     num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[1;32m   1763\u001b[0m )\n\u001b[1;32m   1765\u001b[0m \u001b[39m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m keep_in_memory \u001b[39m=\u001b[39m (\n\u001b[1;32m   1767\u001b[0m     keep_in_memory \u001b[39mif\u001b[39;00m keep_in_memory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m is_small_dataset(builder_instance\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size)\n\u001b[1;32m   1768\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/builder.py:860\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    858\u001b[0m     \u001b[39mif\u001b[39;00m num_proc \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m         prepare_split_kwargs[\u001b[39m\"\u001b[39m\u001b[39mnum_proc\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m num_proc\n\u001b[0;32m--> 860\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_download_and_prepare(\n\u001b[1;32m    861\u001b[0m         dl_manager\u001b[39m=\u001b[39;49mdl_manager,\n\u001b[1;32m    862\u001b[0m         verify_infos\u001b[39m=\u001b[39;49mverify_infos,\n\u001b[1;32m    863\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mprepare_split_kwargs,\n\u001b[1;32m    864\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdownload_and_prepare_kwargs,\n\u001b[1;32m    865\u001b[0m     )\n\u001b[1;32m    866\u001b[0m \u001b[39m# Sync info\u001b[39;00m\n\u001b[1;32m    867\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(split\u001b[39m.\u001b[39mnum_bytes \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39msplits\u001b[39m.\u001b[39mvalues())\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/builder.py:953\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verify_infos, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m    949\u001b[0m split_dict\u001b[39m.\u001b[39madd(split_generator\u001b[39m.\u001b[39msplit_info)\n\u001b[1;32m    951\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    952\u001b[0m     \u001b[39m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[0;32m--> 953\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_split(split_generator, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mprepare_split_kwargs)\n\u001b[1;32m    954\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    955\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[1;32m    956\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot find data file. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    957\u001b[0m         \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanual_download_instructions \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    958\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mOriginal error:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    959\u001b[0m         \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)\n\u001b[1;32m    960\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/builder.py:1706\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split\u001b[0;34m(self, split_generator, file_format, num_proc, max_shard_size)\u001b[0m\n\u001b[1;32m   1704\u001b[0m gen_kwargs \u001b[39m=\u001b[39m split_generator\u001b[39m.\u001b[39mgen_kwargs\n\u001b[1;32m   1705\u001b[0m job_id \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1706\u001b[0m \u001b[39mfor\u001b[39;00m job_id, done, content \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_split_single(\n\u001b[1;32m   1707\u001b[0m     gen_kwargs\u001b[39m=\u001b[39mgen_kwargs, job_id\u001b[39m=\u001b[39mjob_id, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_prepare_split_args\n\u001b[1;32m   1708\u001b[0m ):\n\u001b[1;32m   1709\u001b[0m     \u001b[39mif\u001b[39;00m done:\n\u001b[1;32m   1710\u001b[0m         result \u001b[39m=\u001b[39m content\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/builder.py:1849\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[0m\n\u001b[1;32m   1847\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, SchemaInferenceError) \u001b[39mand\u001b[39;00m e\u001b[39m.\u001b[39m__context__ \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1848\u001b[0m         e \u001b[39m=\u001b[39m e\u001b[39m.\u001b[39m__context__\n\u001b[0;32m-> 1849\u001b[0m     \u001b[39mraise\u001b[39;00m DatasetGenerationError(\u001b[39m\"\u001b[39m\u001b[39mAn error occurred while generating the dataset\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m   1851\u001b[0m \u001b[39myield\u001b[39;00m job_id, \u001b[39mTrue\u001b[39;00m, (total_num_examples, total_num_bytes, writer\u001b[39m.\u001b[39m_features, num_shards, shard_lengths)\n",
      "\u001b[0;31mDatasetGenerationError\u001b[0m: An error occurred while generating the dataset"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "'id', 'text', 'lines'\n",
    "\"\"\"\n",
    "#wiki = load_dataset(\"fever\", \"wiki_pages\")\n",
    "wiki = load_dataset('data/wiki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    }
   ],
   "source": [
    "# save for later\n",
    "dataset.save_to_disk(\"data/fever\")\n",
    "# load\n",
    "#datasets = load_dataset('data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nikolaj_Coster-Waldau'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = train[0][\"evidence_wiki_url\"]\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/cas/Documents/coding/python/Aletheia/nli_exploration/multi_nli.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/cas/Documents/coding/python/Aletheia/nli_exploration/multi_nli.ipynb#X40sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m wiki \u001b[39m=\u001b[39m wiki[\u001b[39m\"\u001b[39;49m\u001b[39mwikipedia_pages\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/cas/Documents/coding/python/Aletheia/nli_exploration/multi_nli.ipynb#X40sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m wiki[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "wiki = wiki[\"wikipedia_pages\"]\n",
    "wiki[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess\n",
    "Apply tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 1.18k/1.18k [00:00<00:00, 594kB/s]\n",
      "Downloading: 100%|██████████| 190/190 [00:00<00:00, 96.8kB/s]\n",
      "Downloading: 100%|██████████| 10.6k/10.6k [00:00<00:00, 4.71MB/s]\n",
      "Downloading: 100%|██████████| 612/612 [00:00<00:00, 217kB/s]\n",
      "Downloading: 100%|██████████| 116/116 [00:00<00:00, 61.1kB/s]\n",
      "Downloading: 100%|██████████| 39.3k/39.3k [00:00<00:00, 191kB/s] \n",
      "Downloading: 100%|██████████| 90.9M/90.9M [00:02<00:00, 31.0MB/s]\n",
      "Downloading: 100%|██████████| 53.0/53.0 [00:00<00:00, 21.5kB/s]\n",
      "Downloading: 100%|██████████| 112/112 [00:00<00:00, 47.8kB/s]\n",
      "Downloading: 100%|██████████| 466k/466k [00:00<00:00, 857kB/s] \n",
      "Downloading: 100%|██████████| 350/350 [00:00<00:00, 158kB/s]\n",
      "Downloading: 100%|██████████| 13.2k/13.2k [00:00<00:00, 4.76MB/s]\n",
      "Downloading: 100%|██████████| 232k/232k [00:00<00:00, 376kB/s]  \n",
      "Downloading: 100%|██████████| 349/349 [00:00<00:00, 258kB/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "# load the tranformer to create embeddings\n",
    "st = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'premise'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/cas/Documents/coding/python/Aletheia/nli_exploration/multi_nli.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/cas/Documents/coding/python/Aletheia/nli_exploration/multi_nli.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m embeds \u001b[39m=\u001b[39m st\u001b[39m.\u001b[39mencode([train[\u001b[39m0\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mpremise\u001b[39;49m\u001b[39m'\u001b[39;49m], train[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mhypothesis\u001b[39m\u001b[39m'\u001b[39m]], convert_to_tensor\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/cas/Documents/coding/python/Aletheia/nli_exploration/multi_nli.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# creates embeddings for both sentences in the array and returns array of size 2\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/cas/Documents/coding/python/Aletheia/nli_exploration/multi_nli.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/cas/Documents/coding/python/Aletheia/nli_exploration/multi_nli.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m#util.pytorch_cos_sim(embeds[0], embeds[1])\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/cas/Documents/coding/python/Aletheia/nli_exploration/multi_nli.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((embeds[\u001b[39m0\u001b[39m], embeds[\u001b[39m1\u001b[39m]))\n",
      "\u001b[0;31mKeyError\u001b[0m: 'premise'"
     ]
    }
   ],
   "source": [
    "embeds = st.encode([train[0]['premise'], train[0]['hypothesis']], convert_to_tensor=True)\n",
    "# creates embeddings for both sentences in the array and returns array of size 2\n",
    "\n",
    "#util.pytorch_cos_sim(embeds[0], embeds[1])\n",
    "x = torch.cat((embeds[0], embeds[1]))\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first ten rows for testing\n",
    "sample = train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/393 [00:00<?, ?ba/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/cas/Documents/coding/python/Aletheia/nli_exploration/multi_nli.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cas/Documents/coding/python/Aletheia/nli_exploration/multi_nli.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         embeds\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39mcat((embeds[\u001b[39m0\u001b[39m], embeds[\u001b[39m1\u001b[39m]))\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cas/Documents/coding/python/Aletheia/nli_exploration/multi_nli.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m {\u001b[39m'\u001b[39m\u001b[39membed\u001b[39m\u001b[39m'\u001b[39m: embeds}\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/cas/Documents/coding/python/Aletheia/nli_exploration/multi_nli.ipynb#X12sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m tk_train \u001b[39m=\u001b[39m train\u001b[39m.\u001b[39;49mmap(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cas/Documents/coding/python/Aletheia/nli_exploration/multi_nli.ipynb#X12sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     tokenize, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cas/Documents/coding/python/Aletheia/nli_exploration/multi_nli.ipynb#X12sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     batched\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cas/Documents/coding/python/Aletheia/nli_exploration/multi_nli.ipynb#X12sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:2826\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   2823\u001b[0m disable_tqdm \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled()\n\u001b[1;32m   2825\u001b[0m \u001b[39mif\u001b[39;00m num_proc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m num_proc \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m-> 2826\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_single(\n\u001b[1;32m   2827\u001b[0m         function\u001b[39m=\u001b[39;49mfunction,\n\u001b[1;32m   2828\u001b[0m         with_indices\u001b[39m=\u001b[39;49mwith_indices,\n\u001b[1;32m   2829\u001b[0m         with_rank\u001b[39m=\u001b[39;49mwith_rank,\n\u001b[1;32m   2830\u001b[0m         input_columns\u001b[39m=\u001b[39;49minput_columns,\n\u001b[1;32m   2831\u001b[0m         batched\u001b[39m=\u001b[39;49mbatched,\n\u001b[1;32m   2832\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m   2833\u001b[0m         drop_last_batch\u001b[39m=\u001b[39;49mdrop_last_batch,\n\u001b[1;32m   2834\u001b[0m         remove_columns\u001b[39m=\u001b[39;49mremove_columns,\n\u001b[1;32m   2835\u001b[0m         keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[1;32m   2836\u001b[0m         load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[1;32m   2837\u001b[0m         cache_file_name\u001b[39m=\u001b[39;49mcache_file_name,\n\u001b[1;32m   2838\u001b[0m         writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[1;32m   2839\u001b[0m         features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m   2840\u001b[0m         disable_nullable\u001b[39m=\u001b[39;49mdisable_nullable,\n\u001b[1;32m   2841\u001b[0m         fn_kwargs\u001b[39m=\u001b[39;49mfn_kwargs,\n\u001b[1;32m   2842\u001b[0m         new_fingerprint\u001b[39m=\u001b[39;49mnew_fingerprint,\n\u001b[1;32m   2843\u001b[0m         disable_tqdm\u001b[39m=\u001b[39;49mdisable_tqdm,\n\u001b[1;32m   2844\u001b[0m         desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[1;32m   2845\u001b[0m     )\n\u001b[1;32m   2846\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2848\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mformat_cache_file_name\u001b[39m(cache_file_name, rank):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:562\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    561\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 562\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    563\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    564\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[1;32m    565\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:529\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    522\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    523\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    524\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    525\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    526\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    527\u001b[0m }\n\u001b[1;32m    528\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 529\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    530\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    531\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/fingerprint.py:480\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[1;32m    478\u001b[0m \u001b[39m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 480\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    482\u001b[0m \u001b[39m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[39mif\u001b[39;00m inplace:  \u001b[39m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:3247\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\u001b[0m\n\u001b[1;32m   3243\u001b[0m indices \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[1;32m   3244\u001b[0m     \u001b[39mrange\u001b[39m(\u001b[39m*\u001b[39m(\u001b[39mslice\u001b[39m(i, i \u001b[39m+\u001b[39m batch_size)\u001b[39m.\u001b[39mindices(input_dataset\u001b[39m.\u001b[39mnum_rows)))\n\u001b[1;32m   3245\u001b[0m )  \u001b[39m# Something simpler?\u001b[39;00m\n\u001b[1;32m   3246\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3247\u001b[0m     batch \u001b[39m=\u001b[39m apply_function_on_filtered_inputs(\n\u001b[1;32m   3248\u001b[0m         batch,\n\u001b[1;32m   3249\u001b[0m         indices,\n\u001b[1;32m   3250\u001b[0m         check_same_num_examples\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(input_dataset\u001b[39m.\u001b[39;49mlist_indexes()) \u001b[39m>\u001b[39;49m \u001b[39m0\u001b[39;49m,\n\u001b[1;32m   3251\u001b[0m         offset\u001b[39m=\u001b[39;49moffset,\n\u001b[1;32m   3252\u001b[0m     )\n\u001b[1;32m   3253\u001b[0m \u001b[39mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[1;32m   3254\u001b[0m     \u001b[39mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[1;32m   3255\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3256\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:3123\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3121\u001b[0m \u001b[39mif\u001b[39;00m with_rank:\n\u001b[1;32m   3122\u001b[0m     additional_args \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (rank,)\n\u001b[0;32m-> 3123\u001b[0m processed_inputs \u001b[39m=\u001b[39m function(\u001b[39m*\u001b[39;49mfn_args, \u001b[39m*\u001b[39;49madditional_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfn_kwargs)\n\u001b[1;32m   3124\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3125\u001b[0m     processed_inputs \u001b[39m=\u001b[39m {\n\u001b[1;32m   3126\u001b[0m         k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mkeys_to_format\n\u001b[1;32m   3127\u001b[0m     }\n",
      "\u001b[1;32m/home/cas/Documents/coding/python/Aletheia/nli_exploration/multi_nli.ipynb Cell 10\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/cas/Documents/coding/python/Aletheia/nli_exploration/multi_nli.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m embeds \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/cas/Documents/coding/python/Aletheia/nli_exploration/multi_nli.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m batch:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/cas/Documents/coding/python/Aletheia/nli_exploration/multi_nli.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     embed \u001b[39m=\u001b[39m st\u001b[39m.\u001b[39mencode(\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/cas/Documents/coding/python/Aletheia/nli_exploration/multi_nli.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         [row[\u001b[39m'\u001b[39;49m\u001b[39mpremise\u001b[39;49m\u001b[39m'\u001b[39;49m], row[\u001b[39m'\u001b[39m\u001b[39mhypothesis\u001b[39m\u001b[39m'\u001b[39m]], \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/cas/Documents/coding/python/Aletheia/nli_exploration/multi_nli.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         convert_to_tensor\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/cas/Documents/coding/python/Aletheia/nli_exploration/multi_nli.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         batch_size\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/cas/Documents/coding/python/Aletheia/nli_exploration/multi_nli.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cas/Documents/coding/python/Aletheia/nli_exploration/multi_nli.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     embeds\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39mcat((embeds[\u001b[39m0\u001b[39m], embeds[\u001b[39m1\u001b[39m]))\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cas/Documents/coding/python/Aletheia/nli_exploration/multi_nli.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mreturn\u001b[39;00m {\u001b[39m'\u001b[39m\u001b[39membed\u001b[39m\u001b[39m'\u001b[39m: embeds}\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "# map to tokenize & embed the entire dataset\n",
    "def tokenize(batch):\n",
    "    embeds = st.encode(\n",
    "        [batch['premise'], batch['hypothesis']], \n",
    "        convert_to_tensor=True,\n",
    "    )\n",
    "    embeds = torch.cat((embeds[0], embeds[1]))\n",
    "    return {'embed': embeds}\n",
    "\n",
    "\n",
    "tk_train = train.map(\n",
    "    tokenize, \n",
    "    batched=True,\n",
    ")\n",
    "#remove_columns=sample.column_names,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['embed'],\n",
       "    num_rows: 768\n",
       "})"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set dataset format to pytorch\n",
    "tk_train.set_format(type=\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"label\"])\n",
    "tk_train.format['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': tensor(1),\n",
       " 'input_ids': tensor([  101, 17158,  2135,  6949,  8301, 25057,  2038,  2048,  3937,  9646,\n",
       "          1011,  4031,  1998, 10505,  1012,   102]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(tk_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "# load pretrained GloVe embeddings\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "max_words = 20\n",
    "embed_len=300\n",
    "global_vectors = GloVe(name='840B', dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# load to GPU for speed up\n",
    "device = 'cuda'\n",
    "# takes string, returns 6000 dim GloVe vector\n",
    "def to_vector(s):\n",
    "    X = tokenizer(s[0])\n",
    "    # fill / cut tokens to max size\n",
    "    if len(X) < max_words:\n",
    "        X = X+[\"\"]*(max_words-len(X))\n",
    "    else:\n",
    "        X = X[:max_words]\n",
    "\n",
    "    X_tensor = torch.zeros(1, max_words, embed_len).to(device)\n",
    "    for i, j in enumerate(X):\n",
    "        X_tensor[0][i] = global_vectors.get_vecs_by_tokens(j)\n",
    "    return(X_tensor.reshape(1, -1))\n",
    "\n",
    "def combined_tensor(row):\n",
    "    x = to_vector(row['premise'])\n",
    "    y = to_vector(row['hypothesis'])\n",
    "\n",
    "    #return torch.cat((x, y), 1)\n",
    "    return {\n",
    "        'vector': torch.cat((x, y), 1),\n",
    "        'label': row['label']\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vector': tensor([[-0.3423, -0.0060, -0.4845,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "        device='cuda:0'),\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_tensor(\n",
    "    dataset['train'][0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset multi_nli (/home/cas/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39)\n"
     ]
    }
   ],
   "source": [
    "sample_data = load_dataset(\"multi_nli\", split='train[:1]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_datta = sample_data.with_transform(\n",
    "    combined_tensor,\n",
    "    columns=['premise', 'hypothesis', 'label'],\n",
    "    output_all_columns=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vector': tensor([-0.0168, -0.2356,  0.0769,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        device='cuda:0'),\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the with_transform applies the custom preprocessing function on-the-fly\n",
    "tf_datta.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map tokenizer to data\n",
    "tk_data = sample_data.map(\n",
    "    # creates a 12k GloVe of premise + hypothesis\n",
    "    combined_tensor,\n",
    "    # speed up by only keeping important columns\n",
    "    input_columns=['premise', 'hypothesis', 'label'],\n",
    "    remove_columns=sample_data.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'vector'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The objectives were realistic.', 'I enjoyed talking with you and must go home. ', 'One of the monsters cried out in English.', 'Road camping trips will be the last thing you have left.'] \n",
      "\n",
      " ['The objectives of our research were to (1) define and describe the characteristics of a worldclass finance organization, (2) identify the factors that are essential for finance organizations to improve their financial management and move towards worldclass standards, and (3) provide case studies which illustrate the efforts of leading finance organizations from private sector companies and state governments to improve their financial management and the overall performance of their organizations.', 'yeah yeah well i need to run i enjoyed talking to you', 'One of the sharp-toothed monsters cried out in a low desert language.', \"and then all you'll have will be road camping trips you know let's just go out to the mountains with the car honey and uh\"] \n",
      "\n",
      " tensor([1, 1, 2, 0])\n"
     ]
    }
   ],
   "source": [
    "# pytorch data loading\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def load_dataloader(split: str, batch: int):\n",
    "    ds = dataset[split].with_format('torch')\n",
    "    dl = DataLoader(ds, batch_size=batch, shuffle=True)   \n",
    "    return dl\n",
    "\n",
    "train_dl = load_dataloader('train', 4)\n",
    "\n",
    "\"\"\"\n",
    "Since we have batch=4, we get 4 datapoints. \n",
    "Each index of the label tensor belongs to one text pair.\n",
    "\"\"\"\n",
    "\n",
    "# test and display dataloader for 1 batch\n",
    "for idx, batch in enumerate(train_dl):\n",
    "    print(batch['hypothesis'], '\\n\\n', batch['premise'], '\\n\\n', batch['label'])\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
